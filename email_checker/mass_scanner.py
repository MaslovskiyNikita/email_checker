import json
import time
import requests
import re
from datetime import datetime
import random
import socket
import concurrent.futures
import os
from bs4 import BeautifulSoup
import threading
from queue import Queue
import logging
from urllib.parse import quote_plus, urlparse
import itertools

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MassWebsiteEmailScanner:
    def __init__(self, proxies=None, max_workers=20):
        self.stats = {
            'total_sites_checked': 0,
            'sites_with_emails': 0,
            'total_emails_found': 0,
            'start_time': datetime.now().isoformat(),
            'sites_processed': 0,
            'search_sites_found': 0
        }
        self.found_emails = set()
        self.visited_urls = set()
        self.proxies = proxies or []
        self.current_proxy = None
        self.session = requests.Session()
        self.max_workers = max_workers
        self.url_queue = Queue()
        self.email_lock = threading.Lock()
        self.stats_lock = threading.Lock()
        self.stop_event = threading.Event()
        
        # –°–ª—É—á–∞–π–Ω—ã–µ User-Agents
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0'
        ]
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–µ—Å—Å–∏–∏
        self.session.headers.update({
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        self.session.timeout = 10
        
        # –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è SSL
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

    def setup_proxy(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–∫—Å–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤"""
        if not self.proxies:
            return None
            
        proxy_str = random.choice(self.proxies)
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ–∫—Å–∏: {proxy_str.split(':')[0]}...")
        
        try:
            if len(proxy_str.split(':')) == 4:
                ip, port, username, password = proxy_str.split(':')
                proxy_url = f"http://{username}:{password}@{ip}:{port}"
            else:
                # –ï—Å–ª–∏ –ø—Ä–æ–∫—Å–∏ –±–µ–∑ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
                proxy_url = f"http://{proxy_str}"
            
            proxies = {
                'http': proxy_url,
                'https': proxy_url
            }
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–æ–∫—Å–∏
            test_response = self.session.get(
                'http://httpbin.org/ip',
                proxies=proxies,
                timeout=5,
                verify=False
            )
            
            if test_response.status_code == 200:
                self.current_proxy = proxies
                logger.info("–ü—Ä–æ–∫—Å–∏ —É—Å–ø–µ—à–Ω–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
                return proxies
            else:
                logger.error("–ü—Ä–æ–∫—Å–∏ –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç")
                return None
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–æ–∫—Å–∏: {e}")
            return None

    def search_sites_by_query(self, query, max_results=100):
        """–ü–æ–∏—Å–∫ —Å–∞–π—Ç–æ–≤ –ø–æ –∫–ª—é—á–µ–≤–æ–º—É –∑–∞–ø—Ä–æ—Å—É —á–µ—Ä–µ–∑ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–æ–∏—Å–∫–æ–≤—ã–µ —Å–∏—Å—Ç–µ–º—ã"""
        logger.info(f"üîç –ü–æ–∏—Å–∫ —Å–∞–π—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'")
        all_domains = set()
        
        # –ú–µ—Ç–æ–¥—ã –ø–æ–∏—Å–∫–∞ —Å –≤–µ—Å–∞–º–∏ (—á–µ–º –±–æ–ª—å—à–µ –≤–µ—Å, —Ç–µ–º —á–∞—â–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)
        search_methods = [
            (self.search_google, 3),
            (self.search_bing, 2),
            (self.search_yandex, 2),
            (self.search_duckduckgo, 1),
        ]
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –º–µ—Ç–æ–¥—ã –ø–æ–∏—Å–∫–∞ –≤ —Å–ª—É—á–∞–π–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
        methods = random.sample(search_methods, len(search_methods))
        
        for search_method, weight in methods:
            if len(all_domains) >= max_results:
                break
                
            try:
                logger.info(f"–ü—Ä–æ–±—É–µ–º {search_method.__name__}...")
                domains = search_method(query, max_results - len(all_domains))
                if domains:
                    all_domains.update(domains)
                    logger.info(f"‚úÖ {search_method.__name__} –Ω–∞—à—ë–ª {len(domains)} —Å–∞–π—Ç–æ–≤")
                    
                    # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∫ —Ä–∞–∑–Ω—ã–º –ø–æ–∏—Å–∫–æ–≤–∏–∫–∞–º
                    time.sleep(random.uniform(1, 3))
                    
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ {search_method.__name__}: {e}")
                continue
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫–æ–≤–∏–∫–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã
        if not all_domains:
            logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ø–æ–∏—Å–∫–∞...")
            backup_domains = self.search_backup_methods(query, max_results)
            all_domains.update(backup_domains)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤
        valid_domains = []
        for domain in list(all_domains)[:max_results]:
            if self.check_domain_exists(domain):
                valid_domains.append(domain)
        
        logger.info(f"‚úÖ –í—Å–µ–≥–æ –≤–∞–ª–∏–¥–Ω—ã—Ö —Å–∞–π—Ç–æ–≤ –ø–æ—Å–ª–µ –ø—Ä–æ–≤–µ—Ä–∫–∏: {len(valid_domains)}")
        
        with self.stats_lock:
            self.stats['search_sites_found'] = len(valid_domains)
            
        return valid_domains

    def search_google(self, query, max_results):
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Google"""
        domains = set()
        try:
            encoded_query = quote_plus(query)
            search_url = f"https://www.google.com/search?q={encoded_query}&num={min(max_results, 50)}"
            
            headers = {
                'User-Agent': random.choice(self.user_agents),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Cache-Control': 'max-age=0',
            }
            
            response = self.session.get(
                search_url,
                headers=headers,
                proxies=self.current_proxy,
                timeout=15,
                verify=False
            )
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # –ò—â–µ–º –æ–±—ã—á–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    if href.startswith('/url?q='):
                        url = href.split('/url?q=')[1].split('&')[0]
                        domain = self.extract_domain(url)
                        if domain and domain not in domains:
                            domains.add(domain)
                            if len(domains) >= max_results:
                                break
                
                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞ —Å—Å—ã–ª–æ–∫
                if not domains:
                    links = re.findall(r'https?://[^\s&]+', response.text)
                    for link in links:
                        domain = self.extract_domain(link)
                        if domain and 'google' not in domain and domain not in domains:
                            domains.add(domain)
                            if len(domains) >= max_results:
                                break
                                
        except Exception as e:
            logger.debug(f"Google search error: {e}")
            
        return list(domains)

    def search_bing(self, query, max_results):
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Bing"""
        domains = set()
        try:
            encoded_query = quote_plus(query)
            search_url = f"https://www.bing.com/search?q={encoded_query}&count={min(max_results, 50)}"
            
            headers = {
                'User-Agent': random.choice(self.user_agents),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
            }
            
            response = self.session.get(
                search_url,
                headers=headers,
                proxies=self.current_proxy,
                timeout=15,
                verify=False
            )
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö Bing
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    if href.startswith('http') and 'bing.com' not in href:
                        domain = self.extract_domain(href)
                        if domain and domain not in domains:
                            domains.add(domain)
                            if len(domains) >= max_results:
                                break
                                
        except Exception as e:
            logger.debug(f"Bing search error: {e}")
            
        return list(domains)

    def search_yandex(self, query, max_results):
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Yandex"""
        domains = set()
        try:
            encoded_query = quote_plus(query)
            search_url = f"https://yandex.ru/search/?text={encoded_query}&numdoc={min(max_results, 50)}"
            
            headers = {
                'User-Agent': random.choice(self.user_agents),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
                'Referer': 'https://yandex.ru/',
            }
            
            response = self.session.get(
                search_url,
                headers=headers,
                proxies=self.current_proxy,
                timeout=15,
                verify=False
            )
            
            if response.status_code == 200:
                # Yandex —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç JavaScript, –Ω–æ –ø–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Å—Å—ã–ª–∫–∏
                links = re.findall(r'https?://[^\s"<>]+', response.text)
                for link in links:
                    if 'yandex' not in link and 'yandex' not in link:
                        domain = self.extract_domain(link)
                        if domain and domain not in domains:
                            domains.add(domain)
                            if len(domains) >= max_results:
                                break
                                
        except Exception as e:
            logger.debug(f"Yandex search error: {e}")
            
        return list(domains)

    def search_duckduckgo(self, query, max_results):
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ DuckDuckGo"""
        domains = set()
        try:
            encoded_query = quote_plus(query)
            search_url = f"https://html.duckduckgo.com/html/?q={encoded_query}"
            
            response = self.session.get(
                search_url,
                proxies=self.current_proxy,
                timeout=15,
                verify=False
            )
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                for link in soup.find_all('a', {'class': 'result__url'}):
                    url = link.text.strip()
                    if url:
                        domain = self.extract_domain(url)
                        if domain and domain not in domains:
                            domains.add(domain)
                            if len(domains) >= max_results:
                                break
                                
        except Exception as e:
            logger.debug(f"DuckDuckGo search error: {e}")
            
        return list(domains)

    def search_backup_methods(self, query, max_results):
        """–†–µ–∑–µ—Ä–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ø–æ–∏—Å–∫–∞ —Å–∞–π—Ç–æ–≤"""
        domains = set()
        
        # 1. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–æ–º–µ–Ω–æ–≤ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        keywords = query.lower().split()
        tlds = ['com', 'net', 'org', 'ru', 'ua', 'kz', 'by']
        
        for keyword in keywords:
            for tld in tlds:
                if len(domains) >= max_results:
                    break
                domains.add(f"{keyword}.{tld}")
        
        # 2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—É–±–ª–∏—á–Ω—ã—Ö –∫–∞—Ç–∞–ª–æ–≥–æ–≤ —Å–∞–π—Ç–æ–≤
        try:
            # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤ DMOZ
            dmoz_urls = [
                f"http://www.dmoz.org/search?q={quote_plus(query)}",
            ]
            for url in dmoz_urls:
                if len(domains) >= max_results:
                    break
                try:
                    response = self.session.get(url, timeout=10, verify=False)
                    if response.status_code == 200:
                        found_domains = re.findall(r'https?://([^/]+)', response.text)
                        for domain in found_domains:
                            if domain and 'dmoz' not in domain:
                                domains.add(domain.replace('www.', ''))
                                if len(domains) >= max_results:
                                    break
                except:
                    continue
        except:
            pass
            
        return list(domains)

    def extract_domain(self, url):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–º–µ–Ω–∞ –∏–∑ URL"""
        try:
            # –ï—Å–ª–∏ —ç—Ç–æ —É–∂–µ –¥–æ–º–µ–Ω, –∞ –Ω–µ URL
            if not url.startswith('http'):
                url = 'http://' + url
                
            parsed = urlparse(url)
            domain = parsed.netloc
            
            # –£–±–∏—Ä–∞–µ–º www
            if domain.startswith('www.'):
                domain = domain[4:]
                
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –¥–æ–º–µ–Ω –≤–∞–ª–∏–¥–Ω—ã–π
            if '.' in domain and len(domain) > 3:
                return domain
        except:
            pass
        return None

    def load_domains_from_sources(self, max_domains=10000):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–º–µ–Ω–æ–≤ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º"""
        domains = set()
        
        # 1. –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—É–±–ª–∏—á–Ω—ã–µ —Å–ø–∏—Å–∫–∏ –¥–æ–º–µ–Ω–æ–≤
        public_domain_sources = [
            "https://raw.githubusercontent.com/opendns/public-domain-lists/master/opendns-top-domains.txt",
            "https://raw.githubusercontent.com/opendns/public-domain-lists/master/opendns-random-domains.txt",
        ]
        
        for source in public_domain_sources:
            try:
                logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–º–µ–Ω–æ–≤ –∏–∑: {source}")
                response = self.session.get(source, timeout=10)
                if response.status_code == 200:
                    lines = response.text.split('\n')
                    for line in lines:
                        if len(domains) >= max_domains:
                            break
                        domain = line.strip().lower()
                        if domain and '.' in domain and not domain.startswith('#'):
                            domains.add(domain)
                    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(domains)} –¥–æ–º–µ–Ω–æ–≤ –∏–∑ {source}")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑ {source}: {e}")
        
        # 2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–æ–º–µ–Ω–æ–≤ –ø–æ —à–∞–±–ª–æ–Ω–∞–º
        needed_domains = max_domains - len(domains)
        if needed_domains > 0:
            logger.info(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è {needed_domains} –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤")
            generated_domains = self.generate_domains(needed_domains)
            domains.update(generated_domains)
        
        # 3. –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ —Ñ–∞–π–ª–∞, –µ—Å–ª–∏ –µ—Å—Ç—å
        try:
            with open('domains.txt', 'r', encoding='utf-8') as f:
                for line in f:
                    if len(domains) >= max_domains:
                        break
                    domain = line.strip()
                    if domain:
                        domains.add(domain)
        except FileNotFoundError:
            pass
            
        logger.info(f"–ò—Ç–æ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–º–µ–Ω–æ–≤: {len(domains)}")
        return list(domains)

    def generate_domains(self, count=10000):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–æ–º–µ–Ω–æ–≤ –ø–æ —à–∞–±–ª–æ–Ω–∞–º"""
        domains = set()
        
        # –ë–∞–∑–æ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        words = [
            'tech', 'digital', 'global', 'smart', 'quick', 'easy', 'fast', 'neo', 
            'meta', 'hyper', 'alpha', 'beta', 'gamma', 'prime', 'elite', 'pro',
            'max', 'ultra', 'mega', 'super', 'net', 'web', 'cloud', 'data',
            'info', 'sys', 'online', 'soft', 'hard', 'code', 'app', 'dev'
        ]
        
        tlds = ['com', 'net', 'org', 'info', 'biz', 'ru', 'ua', 'by', 'kz']
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–±–∏–Ω–∞—Ü–∏–π
        for word1 in words:
            for word2 in words:
                if len(domains) >= count:
                    return list(domains)
                for tld in tlds:
                    if len(domains) >= count:
                        return list(domains)
                    domains.add(f"{word1}{word2}.{tld}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–º–µ–Ω—ã —Å —á–∏—Å–ª–∞–º–∏
        for word in words:
            for i in range(100, 500):
                if len(domains) >= count:
                    return list(domains)
                for tld in ['com', 'net']:
                    if len(domains) >= count:
                        return list(domains)
                    domains.add(f"{word}{i}.{tld}")
        
        return list(domains)

    def check_domain_exists(self, domain):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –¥–æ–º–µ–Ω–∞ —á–µ—Ä–µ–∑ DNS"""
        try:
            socket.gethostbyname(domain)
            return True
        except:
            return False

    def find_existing_domains(self, domains_list, max_domains=5000):
        """–ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –¥–æ–º–µ–Ω–æ–≤"""
        logger.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è {len(domains_list)} –¥–æ–º–µ–Ω–æ–≤...")
        existing_domains = []
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º ThreadPoolExecutor –¥–ª—è DNS –ø—Ä–æ–≤–µ—Ä–æ–∫
        with concurrent.futures.ThreadPoolExecutor(max_workers=min(50, self.max_workers)) as executor:
            future_to_domain = {
                executor.submit(self.check_domain_exists, domain): domain 
                for domain in domains_list[:max_domains * 3]
            }
            
            for future in concurrent.futures.as_completed(future_to_domain):
                if len(existing_domains) >= max_domains:
                    break
                    
                domain = future_to_domain[future]
                try:
                    result = future.result(timeout=5)
                    if result:
                        existing_domains.append(domain)
                        if len(existing_domains) % 100 == 0:
                            logger.info(f"–ù–∞–π–¥–µ–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–æ–º–µ–Ω–æ–≤: {len(existing_domains)}")
                except Exception as e:
                    continue
        
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–æ–º–µ–Ω–æ–≤: {len(existing_domains)}")
        return existing_domains

    def extract_emails_from_text(self, text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ email –∞–¥—Ä–µ—Å–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        emails = re.findall(email_pattern, text)
        return list(set(emails))

    def scan_single_url(self, url):
        """–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–≥–æ URL –Ω–∞ –Ω–∞–ª–∏—á–∏–µ email"""
        if self.stop_event.is_set():
            return
            
        try:
            # –°–ª—É—á–∞–π–Ω—ã–π User-Agent –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
            self.session.headers['User-Agent'] = random.choice(self.user_agents)
            
            # –ü—Ä–æ–±—É–µ–º –æ–±–∞ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞ –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω
            if not url.startswith('http'):
                test_urls = [f'https://{url}', f'http://{url}']
                original_url = url
            else:
                test_urls = [url]
                original_url = url
            
            response = None
            final_url = None
            
            for test_url in test_urls:
                try:
                    logger.debug(f"–ü–æ–ø—ã—Ç–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫: {test_url}")
                    response = self.session.get(
                        test_url,
                        proxies=self.current_proxy,
                        timeout=8,
                        allow_redirects=True,
                        verify=False,
                        stream=True
                    )
                    
                    if response.status_code == 200:
                        final_url = test_url
                        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                        response.raw.decode_content = True
                        content = response.content[:500000]
                        response._content = content
                        break
                    else:
                        logger.debug(f"–°—Ç–∞—Ç—É—Å {response.status_code} –¥–ª—è {test_url}")
                        response = None
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ {test_url}: {e}")
                    continue
            
            if not response or response.status_code != 200:
                with self.stats_lock:
                    self.stats['total_sites_checked'] += 1
                    self.stats['sites_processed'] += 1
                return None
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É
            if response.encoding is None:
                response.encoding = 'utf-8'
            
            try:
                text = response.text
            except UnicodeDecodeError:
                for encoding in ['windows-1251', 'cp1251', 'iso-8859-1']:
                    try:
                        text = response.content.decode(encoding)
                        break
                    except:
                        continue
                else:
                    text = response.content.decode('utf-8', errors='ignore')
            
            emails = self.extract_emails_from_text(text)
            
            # –¢–∞–∫–∂–µ –∏—â–µ–º –≤ —Å—Å—ã–ª–∫–∞—Ö mailto:
            mailto_emails = re.findall(r'mailto:([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,})', text)
            emails.extend(mailto_emails)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            with self.stats_lock:
                self.stats['total_sites_checked'] += 1
                self.stats['sites_processed'] += 1
                
                if emails:
                    self.stats['sites_with_emails'] += 1
                    self.stats['total_emails_found'] += len(emails)
                    with self.email_lock:
                        self.found_emails.update(emails)
                    
                    logger.info(f"üéØ –ù–∞–π–¥–µ–Ω–æ {len(emails)} email –Ω–∞ {original_url}")
                    for email in emails:
                        logger.info(f"  üìß {email}")
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π email —Å—Ä–∞–∑—É
                    self.save_emails_to_file()
                    
                    return {
                        'url': final_url or original_url,
                        'emails': emails,
                        'status': 'success'
                    }
                else:
                    if self.stats['sites_processed'] % 20 == 0:
                        logger.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å–∞–π—Ç–æ–≤: {self.stats['sites_processed']}, –Ω–∞–π–¥–µ–Ω–æ email: {len(self.found_emails)}")
                    return {
                        'url': final_url or original_url,
                        'emails': [],
                        'status': 'no_emails'
                    }
                        
        except Exception as e:
            with self.stats_lock:
                self.stats['total_sites_checked'] += 1
                self.stats['sites_processed'] += 1
            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ {url}: {e}")
            return {
                'url': url,
                'emails': [],
                'status': f'error: {str(e)}'
            }

    def save_emails_to_file(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ email –≤ —Ñ–∞–π–ª"""
        try:
            with open('found_emails.txt', 'w', encoding='utf-8') as f:
                for email in sorted(self.found_emails):
                    f.write(f"{email}\n")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è email: {e}")

    def save_progress(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –≤ JSON"""
        try:
            data = {
                'statistics': self.stats,
                'emails_found': list(self.found_emails),
                'last_update': datetime.now().isoformat()
            }
            with open('scan_progress.json', 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞: {e}")

    def worker(self):
        """–†–∞–±–æ—á–∏–π –ø–æ—Ç–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ URL"""
        while not self.stop_event.is_set():
            try:
                url = self.url_queue.get(timeout=1)
                if url is None:
                    break
                
                self.scan_single_url(url)
                self.url_queue.task_done()
                
            except:
                continue

    def run_mass_scan(self, total_sites=1000, search_query=None):
        """–ó–∞–ø—É—Å–∫ –º–∞—Å—Å–æ–≤–æ–≥–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –ø–æ–∏—Å–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
        logger.info(f"üéØ –ó–∞–ø—É—Å–∫ –º–∞—Å—Å–æ–≤–æ–≥–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ {total_sites} —Å–∞–π—Ç–æ–≤...")
        
        if search_query:
            logger.info(f"üîç –ü–æ–∏—Å–∫ —Å–∞–π—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{search_query}'")
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø—Ä–æ–∫—Å–∏
        if not self.setup_proxy():
            logger.info("‚ö†Ô∏è –†–∞–±–æ—Ç–∞ –±–µ–∑ –ø—Ä–æ–∫—Å–∏")
        
        all_domains = []
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å, —Å–Ω–∞—á–∞–ª–∞ –∏—â–µ–º —Å–∞–π—Ç—ã –ø–æ –Ω–µ–º—É
        if search_query:
            search_domains = self.search_sites_by_query(search_query, max_results=min(200, total_sites))
            all_domains.extend(search_domains)
            logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(search_domains)} —Å–∞–π—Ç–æ–≤ –∏–∑ –ø–æ–∏—Å–∫–∞")
            
            # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–∞–π—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –∏—Ö
            if len(search_domains) >= total_sites:
                existing_domains = search_domains[:total_sites]
            else:
                # –î–æ–ø–æ–ª–Ω—è–µ–º —Å–∞–π—Ç–∞–º–∏ –∏–∑ –¥—Ä—É–≥–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
                remaining_sites = total_sites - len(search_domains)
                other_domains = self.load_domains_from_sources(max_domains=remaining_sites * 2)
                existing_other_domains = self.find_existing_domains(other_domains, max_domains=remaining_sites)
                all_domains.extend(existing_other_domains)
                existing_domains = all_domains
        else:
            # –°—Ç–∞—Ä—ã–π —Ä–µ–∂–∏–º - —Ç–æ–ª—å–∫–æ —Å–ª—É—á–∞–π–Ω—ã–µ —Å–∞–π—Ç—ã
            all_domains = self.load_domains_from_sources(max_domains=total_sites * 2)
            existing_domains = self.find_existing_domains(all_domains, max_domains=total_sites)
        
        if not existing_domains:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–æ–º–µ–Ω—ã")
            return
        
        logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ {len(existing_domains)} —Å–∞–π—Ç–æ–≤...")
        if search_query:
            logger.info(f"üìã –ò–∑ –Ω–∏—Ö –ø–æ –∑–∞–ø—Ä–æ—Å—É '{search_query}': {self.stats['search_sites_found']}")
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ—á–µ—Ä–µ–¥—å URL
        for domain in existing_domains:
            self.url_queue.put(domain)
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º —Ä–∞–±–æ—á–∏–µ –ø–æ—Ç–æ–∫–∏
        threads = []
        worker_count = min(self.max_workers, len(existing_domains))
        
        for i in range(worker_count):
            t = threading.Thread(target=self.worker)
            t.daemon = True
            t.start()
            threads.append(t)
        
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–º –ø—Ä–æ–≥—Ä–µ—Å—Å
        try:
            last_count = 0
            while not self.url_queue.empty():
                current_processed = self.stats['sites_processed']
                
                if current_processed >= last_count + 10:
                    logger.info(f"üìà –ü—Ä–æ–≥—Ä–µ—Å—Å: {current_processed}/{len(existing_domains)} —Å–∞–π—Ç–æ–≤, email: {len(self.found_emails)}")
                    last_count = current_processed
                    self.save_progress()
                
                time.sleep(1)
                
                if current_processed >= len(existing_domains):
                    break
                    
        except KeyboardInterrupt:
            logger.info("‚èπÔ∏è –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            self.stop_event.set()
        finally:
            self.url_queue.join()
            self.stop_event.set()
            time.sleep(2)
        
        self.print_final_stats()

    def print_final_stats(self):
        """–í—ã–≤–æ–¥ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        logger.info("\n" + "="*60)
        logger.info("üìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        logger.info("="*60)
        logger.info(f"–í—Å–µ–≥–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ —Å–∞–π—Ç–æ–≤: {self.stats['total_sites_checked']}")
        if self.stats['search_sites_found'] > 0:
            logger.info(f"–°–∞–π—Ç–æ–≤ –∏–∑ –ø–æ–∏—Å–∫–∞: {self.stats['search_sites_found']}")
        logger.info(f"–°–∞–π—Ç–æ–≤ —Å email: {self.stats['sites_with_emails']}")
        logger.info(f"–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ email: {self.stats['total_emails_found']}")
        logger.info(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö email: {len(self.found_emails)}")
        
        start_time = datetime.fromisoformat(self.stats['start_time'])
        work_time = datetime.now() - start_time
        logger.info(f"–í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã: {work_time}")

if __name__ == "__main__":
    # –ü—Ä–æ–∫—Å–∏ —Å–ø–∏—Å–æ–∫
    proxy_list = [
        "191.102.154.117:9571:D2mBXc:SLokbJ",
        "191.102.172.175:9998:D2mBXc:SLokbJ",
    ]
    
    print("üéØ –ú–ê–°–°–û–í–´–ô –°–ö–ê–ù–ï–† EMAIL –° –ü–û–ò–°–ö–û–ú –ü–û –ó–ê–ü–†–û–°–£")
    print("="*50)
    
    try:
        search_query = input("–í–≤–µ–¥–∏—Ç–µ –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–Ω–∞–ø—Ä–∏–º–µ—Ä '–∫—É–ø–∏—Ç—å –∫–≤–∞—Ä—Ç–∏—Ä—É') –∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –ø—Ä–æ–ø—É—Å–∫–∞: ").strip()
        
        if search_query:
            total_sites = int(input(f"–í–≤–µ–¥–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∞–π—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 500): ") or "500")
        else:
            total_sites = int(input("–í–≤–µ–¥–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∞–π—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏: ") or "1000")
            
        max_workers = int(input("–í–≤–µ–¥–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ (20): ") or "20")
        
        print("\n–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã:")
        print("1 - –° –ø—Ä–æ–∫—Å–∏")
        print("2 - –ë–µ–∑ –ø—Ä–æ–∫—Å–∏")
        
        choice = input("–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä (1 –∏–ª–∏ 2): ").strip()
        
        if choice == "1":
            scanner = MassWebsiteEmailScanner(proxies=proxy_list, max_workers=max_workers)
        else:
            scanner = MassWebsiteEmailScanner(proxies=[], max_workers=max_workers)
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
        scanner.run_mass_scan(total_sites=total_sites, search_query=search_query if search_query else None)
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")